---
title:  "Gaussiano Naive Bayes"
date:   2024-03-12 12:00:00 -500
categories: [Blog]
tags: [data science, machine learning, models, GNB]
---

<!-- Linking MathJax (put this in the header or somewhere at the beginning of your document) -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

![png](https://github.com/gallileugenesis/gallileugenesis.github.io/blob/main/post-img/2024-03-12-GaussianNB/header_image.jpeg?raw=true)


## IntroduÃ§Ã£o

Em Machine Learning, um problema de classificaÃ§Ã£o consiste em prever alguma classe ou rÃ³tulo, com base em um conjunto de dados.

Naive Bayes (NB) Ã© um dos mais antigos algoritmos de classificaÃ§Ã£o utilizados para esse fim, tendo sido desenvolvido na dÃ©cada de 50 do sÃ©culo passado. Ele tem algumas caracterÃ­sticas que o tornam bastante popular: Ã© fÃ¡cil de entender, fÃ¡cil de implementar e apresenta Ã³timos resultados, mesmo para problemas relativamente complexos.

A base matemÃ¡tica de sua aplicaÃ§Ã£o Ã©, como vocÃª deve supor, o teorema de Bayes, que foi proposto no sÃ©culo 18 pelo reverendo inglÃªs [Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes) (1701â€“1761). Ele propÃ´s esse teorema com o humilde objetivo de provar a existÃªncia de Deus (nÃ£o se sabe se ele conseguiu).

No vasto universo do aprendizado de mÃ¡quina, os algoritmos NB se destacam por sua simplicidade, eficiÃªncia e eficÃ¡cia, especialmente em tarefas de classificaÃ§Ã£o. Entre as variantes do Naive Bayes, o Modelo Gaussiano Naive Bayes (GNB) ocupa uma posiÃ§Ã£o de destaque, graÃ§as Ã  sua capacidade de trabalhar diretamente com dados contÃ­nuos, assumindo que os valores de cada caracterÃ­stica sÃ£o distribuÃ­dos segundo uma distribuiÃ§Ã£o Gaussiana ou [normal](https://en.wikipedia.org/wiki/Normal_distribution). 

Este artigo explora em detalhes o funcionamento do GNB, sua fundamentaÃ§Ã£o matemÃ¡tica, caracterÃ­sticas, vantagens e desvantagens. No final, faremos uma aplicaÃ§Ã£o prÃ¡tica com a construÃ§Ã£o do zero em comparaÃ§Ã£o com o modelo da biblioteca scikit-learn.

## Fundamentos MatemÃ¡ticos do GNB

<p>
A ideia bÃ¡sica de um algorÃ­timo de classificaÃ§Ã£o Ã© que ele consiga, com base em conjunto de dados de treinamento \((ğ‘‹,y)\) usado para ajustar o modelo, aprender e atribuir corretamente uma classe para novos valores de entrada. Em outras palavras, um algoritmo de classificaÃ§Ã£o cria uma funÃ§Ã£o matemÃ¡tica \(ğ‘¦=ğ‘“(ğ‘¥)\) que, ajustada pelos dados de treinamento, mapeia um certo conjunto de dados de entrada \(X = [x_1, x_2,...,x_m]\) para um outro conjunto de dados \(y = [y_1,y_2,...,y_K]\), composto por \(K\) classes distintas.

Podemos tratar esse problema de classificaÃ§Ã£o probabilisticamente, avaliando a probabilidade condicional da ocorrÃªncia de uma classe \(ğ‘¦_k\), dado o conjunto de dados \(X\). 
</p>

<p>
Matematicamente, isso pode ser escrito da seguinte forma:
\[
P(y_k|X) = P(y_k|x_1, x_2,...,x_m)
\]

Lemos "A probabilidade de ocorrÃªncia da classe \(ğ‘¦_k\), dado o conjunto de dados \(ğ‘‹\)".
</p>

<p> 
Tudo o que temos que fazer Ã© calcular essa probabilidade para todas as classes em \(y\), e a classe com maior probabilidade Ã© escolhida. 
</p>

<p> 
So easy, nÃ©? Bem...nÃ£o tÃ£o depressa.
</p>

<p> 
A primeira pergunta a ser feita Ã©: como calcular essas probabilidades? Como obter o resultado dessa equaÃ§Ã£o para todas as classes em \(y\)? Ã‰ aqui que entra o reverendo Bayes e seu teorema quase divino.
</p>

<p> 
Aplicando o teorema de Bayes Ã  equaÃ§Ã£o acima, obtemos:
\[
P(y_k|X) = \frac{P(X|y_k)P(y_k)}{P(X)}
\]
</p>

<p> 
ou, de uma forma mais extensa:
\[
P(y_k|X) = \frac{P(x_1, x_2,...,x_m|y_k)P(y_k)}{P(x_1, x_2,...,x_m)}
\]
</p>

<p>
Nesse caso, a probabilidade de interesse, \(ğ‘ƒ(ğ‘¦_k|ğ‘‹)\), Ã© chamada probabilidade a Posteriori, e \(ğ‘ƒ(ğ‘¦_k)\) probabilidade a Priori. JÃ¡ \(ğ‘ƒ(X|ğ‘¦_k)\) Ã© a probabilidade de ocorrÃªncia dos dados de \(ğ‘‹\), se a classe \(ğ‘¦_k\) for verdadeira. Este termo Ã©, por vezes, chamado VerossimilhanÃ§a (Likelihood). E, por fim, \(ğ‘ƒ(X)\) Ã© a probabilidade dos dados de \(X\), independentemente da classe em questÃ£o, tambÃ©m chamado de *EvidÃªncia*.
</p>

<p>
Em termos gerais, \(ğ‘ƒ(ğ‘¦_k)\) pode ser calculada via a frequÃªncia relativa de cada classe no prÃ³prio conjunto de dados de treinamento. No entanto, o cÃ¡lculo da probabilidade conjunta \(P(x_1, x_2,...,x_m|y_k)\) nÃ£o Ã© trivial, pois todas as variÃ¡veis possuem interdependÃªncia e, portanto, precisamos estimar as distribuiÃ§Ãµes de todas as combinaÃ§Ãµes possÃ­veis. Isso requer uma quantidade de dados muito grande o que, por sua vez, aumenta o esforÃ§o computacional necessÃ¡rio para se efetuar o cÃ¡lculo do teorema de Bayes diretamente.
</p>

### SimplificaÃ§Ã£o do Teorema deÂ Bayes

<p>
Bem, como jÃ¡ deu pra notar, a vida Ã© sÃ©ria e a guerra Ã© dura. Precisamos simplificar as coisas para tornar esse cÃ¡lculo viÃ¡vel. Uma simplificaÃ§Ã£o extrema Ã© simplesmente considerar que todos os componentes de \(ğ‘‹\) sÃ£o independentes. Por exemplo, suponha que seu banco de dados (\(X\)) possua medidas diÃ¡rias de temperatura, velocidade do vento e umidade relativa do ar, e seu interesse Ã© prever se irÃ¡ ou nÃ£o chover em um determinado dia. Para aplicar o teorema de Bayes nesse caso, terÃ­amos que supor que essas variÃ¡veis sÃ£o absolutamente independentes uma das outras.
</p>

<p>
Nesse momento vocÃª salta da sua confortÃ¡vel cadeira e branda revoltado "mas considerar a independÃªncia total entre todas as variÃ¡veis de \(ğ‘‹\) Ã© uma suposiÃ§Ã£o bastante forte, tola pra ser mais preciso!". Sim, de fato Ã© verdade, essa consideraÃ§Ã£o Ã© bastante ingÃªnua, jÃ¡ que na prÃ¡tica nÃ£o se pode esperar algo tÃ£o perfeito assim, principalmente em problemas complexos; e Ã© justamente dai que vem o nome do mÃ©todo. No entanto, Ã© surpreendente como o danado funciona bem quando a problemas reais.
</p>

<p>
Com essa consideraÃ§Ã£o, temos entÃ£o o cÃ¡lculo de uma probabilidade condicional com variÃ¡veis independentes, cujo numerador Ã© composto pelas probabilidades condicionais de cada elemento de \(ğ‘‹\) dado um elemento de \(y_k\), assim:

\[
P(y_k|X) = \frac{P(x_1|y_k)P(x_2|y_k)...P(x_m|y_k)P(y_k)}{P(x_1)P(x_2)...P(x_m)}
\]

</p>


O GNB baseia-se no Teorema de Bayes para prever a classe de uma observaÃ§Ã£o. Assume-se que os valores dos atributos seguem uma distribuiÃ§Ã£o gaussiana. A probabilidade de uma caracterÃ­stica, dado que pertence a uma classe especÃ­fica, Ã© modelada pela distribuiÃ§Ã£o Gaussiana:

```
P(x_i|y) = (1 / sqrt(2 * pi * sigma_y^2)) * exp(-((x_i - mu_y)^2 / (2 * sigma_y^2)))
```

Aqui, `mu_y` Ã© a mÃ©dia dos valores de um atributo para a classe `y`, e `sigma_y^2` Ã© a variÃ¢ncia.

## ImplementaÃ§Ã£o e ComparaÃ§Ã£o PrÃ¡tica

Implementamos o GNB 'from scratch' e comparamos seu desempenho com a versÃ£o do scikit-learn usando o dataset Iris. Ambas as implementaÃ§Ãµes alcanÃ§aram uma acurÃ¡cia de 100% na classificaÃ§Ã£o do conjunto de teste, destacando a eficÃ¡cia do GNB mesmo em sua forma mais simples.

## Vantagens do GNB

- **Eficiente com grandes dimensÃµes de dados**
- **Boa performance com um pequeno conjunto de dados**
- **Trata bem dados contÃ­nuos**

## Desvantagens do GNB

- **SuposiÃ§Ã£o de independÃªncia entre os atributos**
- **Sensibilidade a dados nÃ£o representativos**

O GNB se destaca pela sua capacidade de fornecer resultados rÃ¡pidos e precisos, mesmo com suas suposiÃ§Ãµes simplificadoras. Sua aplicaÃ§Ã£o vai desde a classificaÃ§Ã£o de textos atÃ© o reconhecimento de padrÃµes em dados biolÃ³gicos, provando ser uma ferramenta valiosa no arsenal de qualquer cientista de dados.
