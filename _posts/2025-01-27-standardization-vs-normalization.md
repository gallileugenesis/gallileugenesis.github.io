---
title:  "Escalonamento de dados: normaliza√ß√£o vs padroniza√ß√£o"
date:   2025-01-27 12:00:00 -500
categories: [Blog]
tags: [data science, machine learning, pr√©-processamento, normaliza√ß√£o, padroniza√ß√£o]
layout: post
comments: true
---

<!-- Linking MathJax (put this in the header or somewhere at the beginning of your document) -->
<script src="https://polyfill.io/v3/polyfill.min.js?*Features*=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

![png](https://github.com/gallileugenesis/gallileugenesis.github.io/blob/main/post-img/2025-01-27-standardization-vs-normalization/header_image.jpeg?raw=true)


*Features* com maior escala e vari√¢ncia tendem a dominar o aprendizado do modelo, enquanto as de menor escala e vari√¢ncia t√™m menor influ√™ncia.

Modelos baseados em c√°lculos de dist√¢ncias, como o K-Means, KNN e SVM, tendem a ter seus resultados distorcidos pelas dimens√µes com maior escala e vari√¢ncia. Modelos lineares, como a regress√£o log√≠stica ou regress√£o linear, podem levar mais tempo para convergir ou apresentar coeficientes inadequados devido √† escalas desiguais entre as vari√°veis.

Normaliza√ß√£o e padroniza√ß√£o s√£o t√©cnicas de pr√©-processamento para escalonamento de dados. Ou seja, colocam os dados em uma mesma escala, mas de formas diferentes.

**Normaliza√ß√£o:** Reduz os valores para um intervalo fixo, geralmente [0, 1]. 
- M√©todo MinMaxScaler no Scikit-learn.

Propriedades:
1. Ap√≥s a normaliza√ß√£o, os valores estar√£o no intervalo [0,1]:
    - 0 corresponde ao valor m√≠nimo (\min(x)‚Äã).
    - 1 corresponde ao valor m√°ximo (\max(x)).
2. Preserva a distribui√ß√£o relativa dos dados, mas comprime outliers para dentro do intervalo definido.

<p>
\[  
x_{\text{normalizado}} = \frac{x - \min(x)}{\max(x) - \min(x)}
\]
</p>

**Padroniza√ß√£o:** Transforma os dados para uma distribui√ß√£o com m√©dia zero e desvio padr√£o igual a 1. 
- M√©todo StandardScaler no Scikit-learn.

Propriedades:
1. Ap√≥s a padroniza√ß√£o, a distribui√ß√£o dos dados ter√° m√©dia igual a 0 e desvio padr√£o igual a 1.
2. Mant√©m a forma da distribui√ß√£o original, mas ajusta os valores em termos de desvios padr√£o em rela√ß√£o √† m√©dia.
3. N√£o √© sens√≠vel a outliers, mas os outliers permanecem, podendo ter valores padronizados altos ou baixos.

<p>
\[
z = \frac{x - \mu}{\sigma}
\]
</p>

Onde:
- ùë• √© o valor original,
- ùúá √© a m√©dia da distribui√ß√£o,
- ùúé √© o desvio padr√£o.

Abaixo temos um exemplo simples de como fazer a normaliza√ß√£o e padroniza√ß√£o usando a biblioteca sklearn.


```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler 

# Gerando uma distribui√ß√£o de dados aleat√≥rios (100 amostras, 2 *Features*)
np.random.seed(42)
feature1 = np.random.randint(1, 10, size=(100, ))
feature2 = np.random.randint(100, 1000, size=(100, ))

data = np.array([feature1, feature2]).T 
```


```python
# Aplicando MinMaxScaler
max_min_scaler = MinMaxScaler()
normalized_data = max_min_scaler.fit_transform(data)

# Aplicando StandardScaler
standard_scaler = StandardScaler()
standardized_data = standard_scaler.fit_transform(data)
```


```python
# Plotando os dados originais, normalizados e padronizados lado a lado
plt.figure(figsize=(18, 6))

# Gr√°fico dos dados originais
plt.subplot(1, 3, 1)
plt.scatter(data[:, 0], data[:, 1], color='blue', label='Original Data', s=50)
plt.title("Dados Originais", fontsize=18)
plt.xlabel("Feature 1", fontsize=14)
plt.ylabel("Feature 2", fontsize=14)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.grid(alpha=0.5)
plt.xlim(-1000,1000)

# Gr√°fico dos dados normalizados
plt.subplot(1, 3, 2)
plt.scatter(normalized_data[:, 0], normalized_data[:, 1], color='green', label='Normalized Data', s=50)
plt.title("Dados Normalizados", fontsize=18)
plt.xlabel("Feature 1 (Normalized)", fontsize=14)
plt.ylabel("Feature 2 (Normalized)", fontsize=14)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.grid(alpha=0.5)

# Gr√°fico dos dados padronizados
plt.subplot(1, 3, 3)
plt.scatter(standardized_data[:, 0], standardized_data[:, 1], color='red', label='Standardized Data', s=50)
plt.title("Dados Padronizados", fontsize=18)
plt.xlabel("Feature 1 (Standardized)", fontsize=14)
plt.ylabel("Feature 2 (Standardized)", fontsize=14)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.grid(alpha=0.5)

# Ajusta o layout para evitar sobreposi√ß√£o e salva o gr√°fico
plt.tight_layout()
plt.savefig("MinMaxScaler_StandardScaler.png")
plt.show()
```


![png](https://github.com/gallileugenesis/gallileugenesis.github.io/blob/main/post-img/2025-01-27-standardization-vs-normalization/output_8_0.png?raw=true)
  

A primeira figura da esquerda mostra a visualiza√ß√£o dos dados com os eixos variando na mesma magnitude da feature de maior escala (feature 2). Claramente a visualiza√ß√£o fica prejudicada pela diferen√ßa de escala entre as *Features*. Essa diferen√ßa tamb√©m seria prejudicial para alguns modelos de machine learning, como j√° discutido.

No gr√°fico do meio temos os dados normalizados, agora ambas as *Features* variam entre 0 e 1. Isso retira o efeito da escala. Do mesmo modo, a escala j√° n√£o tem impacto no gr√°fico da direita, onde os dados foram padronizando, passando a ter m√©dia zero e desvio padr√£o 1. 