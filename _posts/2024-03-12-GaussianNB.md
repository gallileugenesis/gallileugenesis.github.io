---
title:  "Tudo o que vocÃª precisa saber sobre o algoritmo Naive Bayes"
date:   2024-03-12 12:00:00 -500
categories: [Blog]
tags: [data science, machine learning, models, GNB]
---

<!-- Linking MathJax (put this in the header or somewhere at the beginning of your document) -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

![png](https://github.com/gallileugenesis/gallileugenesis.github.io/blob/main/post-img/2024-03-12-GaussianNB/header_image.jpeg?raw=true)


## IntroduÃ§Ã£o

Em Machine Learning, um problema de classificaÃ§Ã£o consiste em prever alguma classe ou rÃ³tulo, com base em um conjunto de dados.

Naive Bayes (NB) Ã© um dos mais antigos algoritmos de classificaÃ§Ã£o utilizados para esse fim, tendo sido desenvolvido na dÃ©cada de 50 do sÃ©culo passado. Ele tem algumas caracterÃ­sticas que o tornam bastante popular: Ã© fÃ¡cil de entender, fÃ¡cil de implementar e apresenta Ã³timos resultados, mesmo para problemas relativamente complexos.

A base matemÃ¡tica de sua aplicaÃ§Ã£o Ã©, como vocÃª deve supor, o teorema de Bayes, que foi proposto no sÃ©culo 18 pelo reverendo inglÃªs [Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes) (1701â€“1761). Ele propÃ´s esse teorema com o humilde objetivo de provar a existÃªncia de Deus (nÃ£o se sabe se ele conseguiu).

No vasto universo do aprendizado de mÃ¡quina, os algoritmos NB se destacam por sua simplicidade, eficiÃªncia e eficÃ¡cia, especialmente em tarefas de classificaÃ§Ã£o. Entre as variantes do Naive Bayes, o Modelo Gaussiano Naive Bayes (GNB) ocupa uma posiÃ§Ã£o de destaque, graÃ§as Ã  sua capacidade de trabalhar diretamente com dados contÃ­nuos, assumindo que os valores de cada caracterÃ­stica sÃ£o distribuÃ­dos segundo uma distribuiÃ§Ã£o Gaussiana ou [normal](https://en.wikipedia.org/wiki/Normal_distribution). 

Este artigo explora em detalhes o funcionamento do GNB, sua fundamentaÃ§Ã£o matemÃ¡tica, caracterÃ­sticas, vantagens e desvantagens. No final, faremos uma aplicaÃ§Ã£o prÃ¡tica com a construÃ§Ã£o do zero em comparaÃ§Ã£o com o modelo da biblioteca scikit-learn.

## Fundamentos MatemÃ¡ticos do NB

<p>
A ideia bÃ¡sica de um algorÃ­timo de classificaÃ§Ã£o Ã© que ele consiga, com base em conjunto de dados de treinamento \((ğ‘‹,y)\) usado para ajustar o modelo, aprender e atribuir corretamente uma classe para novos valores de entrada. Em outras palavras, um algoritmo de classificaÃ§Ã£o cria uma funÃ§Ã£o matemÃ¡tica \(ğ‘¦=ğ‘“(ğ‘¥)\) que, ajustada pelos dados de treinamento, mapeia um certo conjunto de dados de entrada \(X = [x_1, x_2,...,x_m]\) para um outro conjunto de dados \(y = [y_1,y_2,...,y_K]\), composto por \(K\) classes distintas.

Podemos tratar esse problema de classificaÃ§Ã£o probabilisticamente, avaliando a probabilidade condicional da ocorrÃªncia de uma classe \(ğ‘¦_k\), dado o conjunto de dados \(X\). 
</p>

<p>
Matematicamente, isso pode ser escrito da seguinte forma:
\[
P(y_k|X) = P(y_k|x_1, x_2,...,x_m)
\]

Lemos "A probabilidade de ocorrÃªncia da classe \(ğ‘¦_k\), dado o conjunto de dados \(ğ‘‹\)".
</p>

<p> 
Tudo o que temos que fazer Ã© calcular essa probabilidade para todas as classes em \(y\), e a classe com maior probabilidade Ã© escolhida. 
</p>

<p> 
So easy, nÃ©? Bem...nÃ£o tÃ£o depressa.
</p>

<p> 
A primeira pergunta a ser feita Ã©: como calcular essas probabilidades? Como obter o resultado dessa equaÃ§Ã£o para todas as classes em \(y\)? Ã‰ aqui que entra o reverendo Bayes e seu teorema quase divino.
</p>

<p> 
Aplicando o teorema de Bayes Ã  equaÃ§Ã£o acima, obtemos:
\[
P(y_k|X) = \frac{P(X|y_k)P(y_k)}{P(X)}
\]
</p>

<p> 
ou, de uma forma mais extensa:
\[
P(y_k|X) = \frac{P(x_1, x_2,...,x_m|y_k)P(y_k)}{P(x_1, x_2,...,x_m)}
\]
</p>

<p>
Nesse caso, a probabilidade de interesse, \(ğ‘ƒ(ğ‘¦_k|ğ‘‹)\), Ã© chamada probabilidade a Posteriori, e \(ğ‘ƒ(ğ‘¦_k)\) probabilidade a Priori. JÃ¡ \(ğ‘ƒ(X|ğ‘¦_k)\) Ã© a probabilidade de ocorrÃªncia dos dados de \(ğ‘‹\), se a classe \(ğ‘¦_k\) for verdadeira. Este termo Ã©, por vezes, chamado VerossimilhanÃ§a (Likelihood). E, por fim, \(ğ‘ƒ(X)\) Ã© a probabilidade dos dados de \(X\), independentemente da classe em questÃ£o, tambÃ©m chamado de *EvidÃªncia*.
</p>

<p>
Em termos gerais, \(ğ‘ƒ(ğ‘¦_k)\) pode ser calculada via a frequÃªncia relativa de cada classe no prÃ³prio conjunto de dados de treinamento. No entanto, o cÃ¡lculo da probabilidade conjunta \(P(x_1, x_2,...,x_m|y_k)\) nÃ£o Ã© trivial, pois todas as variÃ¡veis possuem interdependÃªncia e, portanto, precisamos estimar as distribuiÃ§Ãµes de todas as combinaÃ§Ãµes possÃ­veis. Isso requer uma quantidade de dados muito grande o que, por sua vez, aumenta o esforÃ§o computacional necessÃ¡rio para se efetuar o cÃ¡lculo do teorema de Bayes diretamente.
</p>

### SimplificaÃ§Ã£o do Teorema deÂ Bayes

<p>
Bem, como jÃ¡ deu pra notar, a vida Ã© sÃ©ria e a guerra Ã© dura. Precisamos simplificar as coisas para tornar esse cÃ¡lculo viÃ¡vel. Uma simplificaÃ§Ã£o extrema Ã© simplesmente considerar que todos os componentes de \(ğ‘‹\) sÃ£o independentes. Por exemplo, suponha que seu banco de dados (\(X\)) possua medidas diÃ¡rias de temperatura, velocidade do vento e umidade relativa do ar, e seu interesse Ã© prever se irÃ¡ ou nÃ£o chover em um determinado dia. Para aplicar o teorema de Bayes nesse caso, terÃ­amos que supor que essas variÃ¡veis sÃ£o absolutamente independentes uma das outras.
</p>

<p>
Nesse momento vocÃª salta da sua confortÃ¡vel cadeira e branda revoltado "mas considerar a independÃªncia total entre todas as variÃ¡veis de \(ğ‘‹\) Ã© uma suposiÃ§Ã£o bastante forte, tola pra ser mais preciso!". Sim, de fato Ã© verdade, essa consideraÃ§Ã£o Ã© bastante ingÃªnua, jÃ¡ que na prÃ¡tica nÃ£o se pode esperar algo tÃ£o perfeito assim, principalmente em problemas complexos; e Ã© justamente dai que vem o nome do mÃ©todo. No entanto, Ã© surpreendente como o danado funciona bem quando a problemas reais.
</p>

<p>
Com essa consideraÃ§Ã£o, temos entÃ£o o cÃ¡lculo de uma probabilidade condicional com variÃ¡veis independentes, cujo numerador Ã© composto pelas probabilidades condicionais de cada elemento de \(ğ‘‹\) dado um elemento de \(y_k\), assim:

\[
P(y_k|X) = \frac{P(x_1|y_k)P(x_2|y_k)...P(x_m|y_k)P(y_k)}{P(x_1)P(x_2)...P(x_m)}
\]

ou, de uma forma mais chique:
\[
P(y_k|X) = \dfrac{P(y_k) \prod_{i=1}^{m}P(x_i|y_k)}{P(x_1)P(x_2)...P(x_m)}
\]

O denominador da expressÃ£o acima Ã© constante para o cÃ¡lculo das probabilidades condicionais de todas as \(K\) classes em \(y\). Logo, por uma questÃ£o de economia computacional, pode-se omitir essa parcela dos cÃ¡lculos.
</p>

<p>
Nesse caso, dizemos que \(ğ‘ƒ(ğ‘¦_k|ğ‘‹)\) Ã© proporcional a \(ğ‘ƒ(x_1|ğ‘¦_k)ğ‘ƒ(x_2|ğ‘¦_k)â€¦ğ‘ƒ(x_m|ğ‘¦_k)P(y_k)\). 

Matematicamente, escrevemos:

\[
P(y_k|X) \propto P(x_1|y_k)P(x_2|y_k)...P(x_m|y_k)P(y_k)
\]

ou, como antes:
\[
P(y_k|X) \propto P(y_k)\prod_{i=1}^{m}P(x_i|y_k)
\]

Pronto, com isso nÃ³s temos nosso classificador probabilÃ­stico Bayesiano hiper master plus+.
</p>

### MÃ¡xima Probabilidade a Posteriori

<p>
Como jÃ¡ comentamos, essa probabilidade condicional (probabilidade a posteriori) Ã© realizada para todas as \(K\) classes do nosso conjunto de dados e a classe com maior probabilidade Ã© entÃ£o selecionada. Tecnicamente, esse procedimento Ã© chamado de mÃ¡xima probabilidade a posteriori, ou MAP, na sigla em inglÃªs.
</p>

<p>
Sendo assim, a classe predita pelo modelo serÃ¡ dada por:
\[
\widehat{y} = \arg\max_{y_k, k \in 1,2,...,K} P(y_k|X) = \arg\max_{y_k} P(y_k)\prod_{i=1}^{m}P(x_i|y_k)
\]

Em muitas ocasiÃµes, principalmente quando temos uma grande quantidade de dados, Ã© conveniente expressarmos as probabilidades da equaÃ§Ã£o anterior na forma de logaritmo.

\[
\widehat{y} = \arg\max_{y_k} [ln(P(y_k)\prod_{i=1}^{m}P(x_i|y_k))]
\]

Como o logaritmo do produto Ã© igual Ã  soma dos logaritmos, ficamos com:

\[
\widehat{y} = \arg\max_{y_k} [ln P(y_k) + \prod_{i=1}^{m}lnP(x_i|y_k)]
\]

Reparem que trocamos o produto de probabilidades por somas de probabilidades, o que Ã© computacionalmente mais eficiente.
</p>

<p>
EntÃ£o, em resumo, tudo o que precisamos calcular para treinar o modelo sÃ£o as probabilidade envolvidas na equaÃ§Ã£o acima. NÃ£o hÃ¡ coeficientes que precisem ser ajustados via alguma algoritmo de otimizaÃ§Ã£o, como Ã© comum em outros algoritmos de Machine Learning. ConsequÃªncia? Temos um algoritmo de aprendizagem rÃ¡pida e fÃ¡cil implementaÃ§Ã£o.
</p>

## Tipos de algoritmos NB

<p>
Para aplicar o algoritmo NB em problemas reais, precisamos estimar uma distribuiÃ§Ã£o de probabilidades para os recursos de \(X\). Essa estimativa pode ser guiada pelo tipo de dado que compÃµe o recurso. Se as variÃ¡vel sÃ£o categÃ³ricas binÃ¡rias, usa-se uma distribuiÃ§Ã£o de bernoulli para representa-las. Caso haja multiclasses, usa-se uma distribuiÃ§Ã£o multinomial. E, por fim, se estivermos lidando com dados contÃ­nuos, usa-se uma distribuiÃ§Ã£o Gaussiana.
</p>
<p>
E disto surgem os trÃªs tipos de classificadores Naive Bayes:
<ul>
    <li>Bernoulli naive Bayes (BNB).</li>
    <li>Multinomial naive Bayes (MNB).</li>
    <li>Gaussian naive Bayes (GNB).</li>
</ul>

Ã‰ importante frisar que, caso \(X\) possua recursos com diferentes tipos de dados, pode-se atribuir diferentes distribuiÃ§Ãµes de probabilidades para cada um deles. 
</p>

<p>
Outro ponto importante Ã© que, apesar de serem as distribuiÃ§Ãµes mais comumente usadas em um classificador Naive Bayes, se os dados de entrada sÃ£o melhores descritos por outra distribuiÃ§Ã£o de probabilidade, deve-se usÃ¡-la. Ou ainda, se nÃ£o temos certeza sobre a distribuiÃ§Ã£o de probabilidade que melhor descreve esses dados, pode-se usar um estimador de distribuiÃ§Ã£o de probabilidades, tambÃ©m chamado, KDE (Kernel density estimation).
</p>

## Gaussian Naive Bayes

Como discutido anteriormente, a depender da natureza dos recursos de \(X\), pode-se assumir que estes sigam determinada distribuiÃ§Ã£o de probabilidade. Quando se trata de dados contÃ­nuos, na maioria das vezes, assume-se que estes seguem uma distribuiÃ§Ã£o de probabilidade Gaussiana. Nesse caso, as probabilidades na parcela do somatÃ³rio na EquaÃ§Ã£o 1, pode ser obtida pela EquaÃ§Ã£o 2 abaixo:

\[
P(x_i|y) = (1 / sqrt(2 * pi * sigma_y^2)) * exp(-((x_i - mu_y)^2 / (2 * sigma_y^2)))
\]

Aqui, `mu_y` Ã© a mÃ©dia dos valores de um atributo para a classe `y`, e `sigma_y^2` Ã© a variÃ¢ncia.


## Vantagens do GNB

- **Eficiente com grandes dimensÃµes de dados**
- **Boa performance com um pequeno conjunto de dados**
- **Trata bem dados contÃ­nuos**

## Desvantagens do GNB

- **SuposiÃ§Ã£o de independÃªncia entre os atributos**
- **Sensibilidade a dados nÃ£o representativos**

