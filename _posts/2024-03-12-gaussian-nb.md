---
title:  "Tudo o que voc√™ precisa saber sobre o algoritmo Naive Bayes"
date:   2024-03-12 12:00:00 -500
categories: [Blog]
tags: [data science, machine learning, modelos, GNB]
layout: post
comments: true
---

<!-- Linking MathJax (put this in the header or somewhere at the beginning of your document) -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

![png](https://github.com/gallileugenesis/gallileugenesis.github.io/blob/main/post-img/2024-03-12-GaussianNB/header_image.jpeg?raw=true)


## Introdu√ß√£o

Em Machine Learning, um problema de classifica√ß√£o consiste em prever alguma classe ou r√≥tulo, com base em um conjunto de dados.

Naive Bayes (NB) √© um dos mais antigos algoritmos de classifica√ß√£o utilizados para esse fim, tendo sido desenvolvido na d√©cada de 50 do s√©culo passado. Ele tem algumas caracter√≠sticas que o tornam bastante popular: √© f√°cil de entender, f√°cil de implementar e apresenta √≥timos resultados, mesmo para problemas relativamente complexos.

A base matem√°tica de sua aplica√ß√£o √©, como voc√™ deve supor, o teorema de Bayes, que foi proposto no s√©culo 18 pelo reverendo ingl√™s [Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes) (1701‚Äì1761). Ele prop√¥s esse teorema com o humilde objetivo de provar a exist√™ncia de Deus (n√£o se sabe se ele conseguiu).

No vasto universo do aprendizado de m√°quina, os algoritmos NB se destacam por sua simplicidade, efici√™ncia e efic√°cia, especialmente em tarefas de classifica√ß√£o. Entre as variantes do Naive Bayes, o Modelo Gaussiano Naive Bayes (GNB) ocupa uma posi√ß√£o de destaque, gra√ßas √† sua capacidade de trabalhar diretamente com dados cont√≠nuos, assumindo que os valores de cada caracter√≠stica s√£o distribu√≠dos segundo uma distribui√ß√£o Gaussiana ou [normal](https://en.wikipedia.org/wiki/Normal_distribution). 

Este artigo explora em detalhes o funcionamento do NB, sua fundamenta√ß√£o matem√°tica, caracter√≠sticas, vantagens e desvantagens. 

## Fundamentos Matem√°ticos do NB

<p>
A ideia b√°sica de um algor√≠timo de classifica√ß√£o √© que ele consiga, com base em conjunto de dados de treinamento \((ùëã,y)\) usado para ajustar o modelo, aprender e atribuir corretamente uma classe para novos valores de entrada. Em outras palavras, um algoritmo de classifica√ß√£o cria uma fun√ß√£o matem√°tica \(ùë¶=ùëì(ùë•)\) que, ajustada pelos dados de treinamento, mapeia um certo conjunto de dados de entrada \(X = [x_1, x_2,...,x_m]\) para um outro conjunto de dados \(y = [y_1,y_2,...,y_K]\), composto por \(K\) classes distintas.

Podemos tratar esse problema de classifica√ß√£o probabilisticamente, avaliando a probabilidade condicional da ocorr√™ncia de uma classe \(ùë¶_k\), dado o conjunto de dados \(X\). 
</p>

<p>
Matematicamente, isso pode ser escrito da seguinte forma:
\[
P(y_k|X) = P(y_k|x_1, x_2,...,x_m)
\]

Lemos "A probabilidade de ocorr√™ncia da classe \(ùë¶_k\), dado o conjunto de dados \(ùëã\)".
</p>

<p> 
Tudo o que temos que fazer √© calcular essa probabilidade para todas as classes em \(y\), e a classe com maior probabilidade √© escolhida. 
</p>

<p> 
So easy, n√©? Bem...n√£o t√£o depressa.
</p>

<p> 
A primeira pergunta a ser feita √©: como calcular essas probabilidades? Como obter o resultado dessa equa√ß√£o para todas as classes em \(y\)? √â aqui que entra o reverendo Bayes e seu teorema quase divino.
</p>

<p> 
Aplicando o teorema de Bayes √† equa√ß√£o acima, obtemos:
\[
P(y_k|X) = \frac{P(X|y_k)P(y_k)}{P(X)}
\]
</p>

<p> 
ou, de uma forma mais extensa:
\[
P(y_k|X) = \frac{P(x_1, x_2,...,x_m|y_k)P(y_k)}{P(x_1, x_2,...,x_m)}
\]
</p>

<p>
Nesse caso, a probabilidade de interesse, \(ùëÉ(ùë¶_k|ùëã)\), √© chamada probabilidade a Posteriori, e \(ùëÉ(ùë¶_k)\) probabilidade a Priori. J√° \(ùëÉ(X|ùë¶_k)\) √© a probabilidade de ocorr√™ncia dos dados de \(ùëã\), se a classe \(ùë¶_k\) for verdadeira. Este termo √©, por vezes, chamado Verossimilhan√ßa (Likelihood). E, por fim, \(ùëÉ(X)\) √© a probabilidade dos dados de \(X\), independentemente da classe em quest√£o, tamb√©m chamado de Evid√™ncia.
</p>

<p>
Em termos gerais, \(ùëÉ(ùë¶_k)\) pode ser calculada via a frequ√™ncia relativa de cada classe no pr√≥prio conjunto de dados de treinamento. No entanto, o c√°lculo da probabilidade conjunta \(P(x_1, x_2,...,x_m|y_k)\) n√£o √© trivial, pois todas as vari√°veis possuem interdepend√™ncia e, portanto, precisamos estimar as distribui√ß√µes de todas as combina√ß√µes poss√≠veis. Isso requer uma quantidade de dados muito grande o que, por sua vez, aumenta o esfor√ßo computacional necess√°rio para se efetuar o c√°lculo do teorema de Bayes diretamente.
</p>

### Simplifica√ß√£o do Teorema de¬†Bayes

<p>
Bem, como j√° deu pra notar, a vida √© s√©ria e a guerra √© dura. Precisamos simplificar as coisas para tornar esse c√°lculo vi√°vel. Uma simplifica√ß√£o extrema √© simplesmente considerar que todos os componentes de \(ùëã\) s√£o independentes. Por exemplo, suponha que seu banco de dados (\(X\)) possua medidas di√°rias de temperatura, velocidade do vento e umidade relativa do ar, e seu interesse √© prever se ir√° ou n√£o chover em um determinado dia. Para aplicar o teorema de Bayes nesse caso, ter√≠amos que supor que essas vari√°veis s√£o absolutamente independentes uma das outras.
</p>

<p>
Nesse momento voc√™ salta da sua confort√°vel cadeira e branda revoltado "mas considerar a independ√™ncia total entre todas as vari√°veis de \(ùëã\) √© uma suposi√ß√£o bastante forte, tola pra ser mais preciso!". Sim, de fato √© verdade, essa considera√ß√£o √© bastante ing√™nua, j√° que na pr√°tica n√£o se pode esperar algo t√£o perfeito assim, principalmente em problemas complexos; e √© justamente dai que vem o nome do m√©todo. No entanto, √© surpreendente como o danado funciona bem quando a problemas reais.
</p>

<p>
Com essa considera√ß√£o, temos ent√£o o c√°lculo de uma probabilidade condicional com vari√°veis independentes, cujo numerador √© composto pelas probabilidades condicionais de cada elemento de \(ùëã\) dado um elemento de \(y_k\), assim:

\[
P(y_k|X) = \frac{P(x_1|y_k)P(x_2|y_k)...P(x_m|y_k)P(y_k)}{P(x_1)P(x_2)...P(x_m)}
\]

ou, de uma forma mais chique:
\[
P(y_k|X) = \dfrac{P(y_k) \prod_{i=1}^{m}P(x_i|y_k)}{P(x_1)P(x_2)...P(x_m)}
\]

O denominador da express√£o acima √© constante para o c√°lculo das probabilidades condicionais de todas as \(K\) classes em \(y\). Logo, por uma quest√£o de economia computacional, pode-se omitir essa parcela dos c√°lculos.
</p>

<p>
Nesse caso, dizemos que \(ùëÉ(ùë¶_k|ùëã)\) √© proporcional a \(ùëÉ(x_1|ùë¶_k)ùëÉ(x_2|ùë¶_k)‚Ä¶ùëÉ(x_m|ùë¶_k)P(y_k)\). 

Matematicamente, escrevemos:

\[
P(y_k|X) \propto P(x_1|y_k)P(x_2|y_k)...P(x_m|y_k)P(y_k)
\]

ou, como antes:
\[
P(y_k|X) \propto P(y_k)\prod_{i=1}^{m}P(x_i|y_k)
\]

Pronto, com isso n√≥s temos nosso classificador probabil√≠stico Bayesiano hiper master plus+.
</p>

### M√°xima Probabilidade a Posteriori

<p>
Como j√° comentamos, essa probabilidade condicional (probabilidade a posteriori) √© realizada para todas as \(K\) classes do nosso conjunto de dados e a classe com maior probabilidade √© ent√£o selecionada. Tecnicamente, esse procedimento √© chamado de m√°xima probabilidade a posteriori, ou MAP, na sigla em ingl√™s.
</p>

<p>
Sendo assim, a classe predita pelo modelo ser√° dada por:
\[
\widehat{y} = \arg\max_{y_k} P(y_k|X) = \arg\max_{y_k} P(y_k)\prod_{i=1}^{m}P(x_i|y_k)
\]

Em muitas ocasi√µes, principalmente quando temos uma grande quantidade de dados, √© conveniente expressarmos as probabilidades da equa√ß√£o anterior na forma de logaritmo.

\[
\widehat{y} = \arg\max_{y_k} [ln(P(y_k)\prod_{i=1}^{m}P(x_i|y_k))]
\]

Como o logaritmo do produto √© igual √† soma dos logaritmos, ficamos com:

\[
\widehat{y} = \arg\max_{y_k} [ln (P(y_k)) + \sum_{i=1}^{m}ln(P(x_i|y_k))]
\]

Reparem que trocamos o produto de probabilidades por somas de probabilidades, o que √© computacionalmente mais eficiente.
</p>

<p>
Ent√£o, em resumo, tudo o que precisamos calcular para treinar o modelo s√£o as probabilidade envolvidas na equa√ß√£o acima. N√£o h√° coeficientes que precisem ser ajustados via alguma algoritmo de otimiza√ß√£o, como √© comum em outros algoritmos de Machine Learning. Consequ√™ncia? Temos um algoritmo de aprendizagem r√°pida e f√°cil implementa√ß√£o.
</p>

## Tipos de algoritmos NB

<p>
Para aplicar o algoritmo NB em problemas reais, precisamos estimar uma distribui√ß√£o de probabilidades para os recursos de \(X\). Essa estimativa pode ser guiada pelo tipo de dado que comp√µe o recurso. Se as vari√°vel s√£o categ√≥ricas bin√°rias, usa-se uma distribui√ß√£o de bernoulli para representa-las. Caso haja multiclasses, usa-se uma distribui√ß√£o multinomial. E, por fim, se estivermos lidando com dados cont√≠nuos, usa-se uma distribui√ß√£o Gaussiana.
</p>
<p>
E disto surgem os tr√™s tipos de classificadores Naive Bayes:
<ul>
    <li>Bernoulli naive Bayes (BNB).</li>
    <li>Multinomial naive Bayes (MNB).</li>
    <li>Gaussian naive Bayes (GNB).</li>
</ul>

√â importante frisar que, caso \(X\) possua recursos com diferentes tipos de dados, pode-se atribuir diferentes distribui√ß√µes de probabilidades para cada um deles. 
</p>

<p>
Outro ponto importante √© que, apesar de serem as distribui√ß√µes mais comumente usadas em um classificador Naive Bayes, se os dados de entrada s√£o melhores descritos por outra distribui√ß√£o de probabilidade, deve-se us√°-la. Ou ainda, se n√£o temos certeza sobre a distribui√ß√£o de probabilidade que melhor descreve esses dados, pode-se usar um estimador de distribui√ß√£o de probabilidades, tamb√©m chamado, KDE (Kernel density estimation).
</p>

## Gaussian Naive Bayes

<p>
Como discutido anteriormente, a depender da natureza dos recursos de \(X\), pode-se assumir que estes sigam determinada distribui√ß√£o de probabilidade. Quando se trata de dados cont√≠nuos, na maioria das vezes, assume-se que estes seguem uma distribui√ß√£o de probabilidade Gaussiana. Nesse caso, as probabilidades da  Verossimilhan√ßa (Likelihood) pode ser obtida pela equa√ß√£o abaixo:

\[
P(x_i|y_k) = \dfrac{1}{\sqrt(2 \pi \sigma_k^2)} e^{-\dfrac{(x_i - \mu_k)^2}{2 \sigma_k^2}}
\]

E, desse modo, os √∫nicos par√¢metros que precisamos estimar, a partir dos dados de treinamento, s√£o a m√©dia \(\mu_k\) e o desvio padr√£o \(\sigma_k\) de cada classe \(y_k\).
</p>

<p>
Simples, n√©?
</p>

### Vantagens do GNB

<ul>
    <li>Eficiente com grandes dimens√µes de dados.</li>
    <li>Boa performance com um pequeno conjunto de dados.</li>
    <li>Trata bem dados cont√≠nuos.</li>
</ul>

### Desvantagens do GNB

<ul>
    <li>Suposi√ß√£o de independ√™ncia entre os atributos.</li>
    <li>Sensibilidade a dados n√£o representativos.</li>
</ul>

## Aplica√ß√£o pr√°tica GNB

Para ilustrar a aplica√ß√£o do Modelo Gaussiano Naive Bayes, vamos implement√°-lo "from scratch" e tamb√©m utilizar a biblioteca scikit-learn para classificar o conjunto de dados [Iris](https://archive.ics.uci.edu/dataset/53/iris).

### Implementa√ß√£o *"from Scratch"*
Primeiro, definiremos uma classe GaussianNBFromScratch que calcular√° as m√©dias e vari√¢ncias para cada classe e caracter√≠stica, e usar√° essas estat√≠sticas para fazer previs√µes baseadas na formula√ß√£o matem√°tica apresentada anteriormente.


```python
import numpy as np

class GaussianNBFromScratch:
    """
    Implementa√ß√£o simplificada do Gaussian Naive Bayes para classifica√ß√£o.

    Atributos:
        classes (np.array): Array √∫nico das classes no conjunto de dados.
        parameters (dict): Dicion√°rio contendo par√¢metros (m√©dia, vari√¢ncia e priori) 
                            para cada classe.
    """
    
    def fit(self, X, y):
        """
        Treina o modelo Gaussian Naive Bayes com os dados fornecidos.

        Par√¢metros:
            X (np.array): Conjunto de dados de caracter√≠sticas, onde cada linha 
                          representa uma amostra e cada coluna uma caracter√≠stica.
            y (np.array): Vetor de r√≥tulos de classe para cada amostra no conjunto 
                          de dados X.
        """
        
        self.classes = np.unique(y)  # Identifica e armazena as classes √∫nicas no vetor de r√≥tulos
        self.parameters = {}  # Inicializa o dicion√°rio para armazenar os par√¢metros para cada classe
        
        # Calcula e armazena os par√¢metros para cada classe
        for c in self.classes:
            X_c = X[y == c]  # Seleciona as amostras pertencentes √† classe c
            self.parameters[c] = {
                'mean': X_c.mean(axis=0),  # Calcula a m√©dia para cada caracter√≠stica
                'var': X_c.var(axis=0),  # Calcula a vari√¢ncia para cada caracter√≠stica
                'prior': X_c.shape[0] / X.shape[0]  # Calcula a probabilidade a priori da classe
            }
    
    def predict(self, X):
        """
        Realiza a classifica√ß√£o das amostras fornecidas.

        Par√¢metros:
            X (np.array): Conjunto de dados de caracter√≠sticas a serem classificadas.

        Retorna:
            np.array: Um vetor de r√≥tulos de classe previstos para cada amostra.
        """
        y_pred = [self._predict(x) for x in X]  # Usa a fun√ß√£o auxiliar _predict para cada amostra
        return np.array(y_pred)  # Retorna as previs√µes como um array numpy
    
    def _predict(self, x):
        """
        Auxiliar que calcula a classe mais prov√°vel para uma √∫nica amostra.

        Par√¢metros:
            x (np.array): Uma amostra √∫nica a ser classificada.

        Retorna:
            int: A classe prevista para a amostra.
        """
        posteriors = []  # Lista para armazenar as probabilidades posteriores para cada classe
        
        # Calcula a probabilidade posterior para cada classe
        for c, params in self.parameters.items():
            prior = np.log(params['prior'])  # Log da probabilidade a priori da classe
            conditional = np.sum(np.log(self._pdf(x, params['mean'], params['var'])))  # Log da probabilidade condicional
            posterior = prior + conditional  # Log da probabilidade posterior
            posteriors.append(posterior)
        
        return self.classes[np.argmax(posteriors)]  # Retorna a classe com a maior probabilidade posterior
    
    def _pdf(self, x, mean, var):
        """
        Calcula a fun√ß√£o densidade de probabilidade (PDF) gaussiana.

        Par√¢metros:
            x (float): O valor da caracter√≠stica a ser avaliada.
            mean (float): A m√©dia da caracter√≠stica para a classe.
            var (float): A vari√¢ncia da caracter√≠stica para a classe.

        Retorna:
            float: O valor da PDF gaussiana para x.
        """
        return (1. / np.sqrt(2 * np.pi * var)) * np.exp(-(x - mean) ** 2 / (2 * var))

```

### Banco de dados Iris


A base de dados [Iris](https://archive.ics.uci.edu/dataset/53/iris) √© um dos conjuntos de dados mais ic√¥nicos e amplamente utilizados no campo do aprendizado de m√°quina e estat√≠stica. Introduzida pelo estat√≠stico brit√¢nico Ronald Fisher em 1936, ela cont√©m 150 amostras de tr√™s esp√©cies diferentes de flores de √≠ris (Iris setosa, Iris virginica e Iris versicolor), cada uma com 50 amostras. Para cada amostra, s√£o medidas e registradas quatro caracter√≠sticas: o comprimento e a largura das s√©palas e das p√©talas.


```python
from sklearn.datasets import load_iris
import pandas as pd

# Carregar os dados
data = load_iris()

# Criar um dataframe pandas
df = pd.DataFrame(data=data.data, columns=data.feature_names)
df['target'] = data.target

# Exibir as primeiras linhas do DataFrame para verifica√ß√£o
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
import seaborn as sns
import matplotlib.pyplot as plt

# Configurando o tema do Seaborn
sns.set_theme(style="whitegrid")

# Criando um pairplot com o dataset Iris
sns.pairplot(df, hue="target", diag_kind="kde", markers=["o", "s", "D"], palette="bright")

plt.show()
```


    
![png](https://github.com/gallileugenesis/gallileugenesis.github.io/blob/main/post-img/2024-03-12-GaussianNB/output_5_0.png?raw=true)    


### Dividir os dados em treino e teste


```python
from sklearn.model_selection import train_test_split

X = df.drop('target', axis=1)  # Isso remove a coluna 'target', deixando apenas as caracter√≠sticas
y = df['target']  # Isso seleciona apenas a coluna 'target' como o alvo

# Divide dados em treinamento e teste, como um propor√ß√£o de 70 e 30%, respectivamente
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
```

### Modelo *"from scratch"*


```python
# Criar inst√¢ncia do modelo 
model_from_scratch = GaussianNBFromScratch()
# Treinar o modelo
model_from_scratch.fit(X_train, y_train)
```

### Modelo Scikit-learn 


```python
from sklearn.naive_bayes import GaussianNB

# Inicializar e treinar o modelo
model_sklearn = GaussianNB()
model_sklearn.fit(X_train, y_train)
```




<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "‚ñ∏";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "‚ñæ";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;GaussianNB<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.4/modules/generated/sklearn.naive_bayes.GaussianNB.html">?<span>Documentation for GaussianNB</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>GaussianNB()</pre></div> </div></div></div></div>



### Avaliar os modelos 


```python
from sklearn.metrics import accuracy_score

# Fazer previs√µes e avaliar os modelos
y_pred_from_scratch = model_sklearn.predict(X_test)
accuracy_from_scratch = accuracy_score(y_test, y_pred_from_scratch)

y_pred_sklearn = model_sklearn.predict(X_test)
accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)

print(f"Acur√°cia do nosso modelo from scratch: {accuracy_from_scratch:.2f}")
print(f"Acur√°cia do modelo sklearn: {accuracy_sklearn:.2f}")
```

    Acur√°cia do nosso modelo from scratch: 0.98
    Acur√°cia do modelo sklearn: 0.98
    
**Nota:** Todo o c√≥digo est√° dispon√≠vel no [Github](https://github.com/gallileugenesis/gaussian-naive-bayes)